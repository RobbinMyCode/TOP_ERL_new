---
name: "SLURM"
partition: "accelerated"
job-name: "box_random_seq_entire_rand_fixed"
num_parallel_jobs: 120
ntasks: 1
cpus-per-task: 152
time: 2500 #600 for 10 hours
gpus_per_rep: 1
scheduler: horeka
sbatch_args:
  gres: "gpu:4"


experiment_copy_src:
  - "../" # RL Code base
experiment_copy_auto_dst: "../../CODE_COPY/"
---
name: box_random_seq_entire
import_path: "./slurm_dense_permute_n_3_upt_init.yaml"
import_exp: "box_random_seq_entire"




# cw2 config
repetitions: 2      # Number of repetitions, each with different random seed, 2 [for hyper paremtertuning 2] or 4 for test, 20 for paper
reps_per_job: 4     # job id assigment
reps_in_parallel: 4 #jobs run in parallel = # of gpus used
iterations: &iterations 35000
num_checkpoints: 1

# Hardware specific parameters
params:
  sampler:
    args:
      num_env_train: 4
      num_env_test: 38
      episodes_per_train_env: 1
      episodes_per_test_env: 4
  projection:
    args:
      total_train_steps: *iterations

grid:
  reference_split:
    random_permute_splits: [True, False] #e.g. ranges = [12,15,36,0] -> permute that list every iteration before assignment, usable for ALL split_strategies
    ranges: [
      [ 50, 25, 25 ],
      [ 75, 15, 10 ],
      [ 40, 40, 30 ],
      [ 80, 10, 10 ],
      [90, 5, 5],
      [49, 49, 2]
    ]
    #required for fixed_sizes --> number of steps assigned for each policy, len(ranged) = n_splits (else error)





