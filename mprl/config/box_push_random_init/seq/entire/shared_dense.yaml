# Global config
exp_name: &exp_name "box_random_seq_entire"
exp_path: &exp_path "../../mprl_exp_result"
sub_exp_name: &sub_exp_name "q_loss continuing, v_func truncated"
act_func_hidden: &act_func_hidden leaky_relu
act_func_last: &act_func_last null
dtype: &dtype "float32"
device: &device "cuda"
seed: auto

# cw2 config
name: *exp_name
path: *exp_path
verbose_level: 2

# wandb
wandb:
  project: *exp_name
  group: *sub_exp_name
  entity: uiery
  log_interval: &log_interval 100
  log_model: true
  model_name: model

# experiment parameters
params:
  agent:
    type: TopErlAgentMultiProcessing
    # type: TopErlAgent
    args:
      lr_policy: 3e-4
      lr_critic: 5e-5
      wd_policy: 1e-5
      wd_critic: 1e-5
      use_mix_precision: true
      schedule_lr_policy: false
      schedule_lr_critic: false
      entropy_penalty_coef: 0.0
      discount_factor: 1
      epochs_policy: 15 #10-50
      epochs_critic: 30 #10-50
      balance_check: *log_interval
      evaluation_interval: *log_interval
      use_old_policy: true  # use a target policy as the old policy for the trust region projection
      old_policy_update_rate: 0.005  # update rate for the old policy from the current policy
      batch_size: 512
      critic_update_from: 1 # 300
      policy_update_from: 1 # 2000
      dtype: *dtype
      device: *device

  mp: &mp
    type: prodmp
    args:
      num_dof: 7
      tau: 2.0
      alpha_phase: 3
      num_basis: 8
      basis_bandwidth_factor: 3
      num_basis_outside: 0
      alpha: 10
      disable_goal: false
      relative_goal: false
      auto_scale_basis: true
      weights_scale: 0.3
      goal_scale: 0.3
      dt: 0.02
      dtype: *dtype
      device: *device

  policy:
    type: TopErlPolicy
    include_pos_in_forcing_terms: false
    args:
      mean_net_args:
        avg_neuron: 256
        num_hidden: 2
        shape: 0.0
      variance_net_args:
        std_only: false
        contextual: false
      init_method: orthogonal
      out_layer_gain: 0.01
      min_std: 1e-5
      act_func_hidden: *act_func_hidden
      act_func_last: *act_func_last
      dtype: *dtype
      device: *device
      mp: *mp

  reference_split:
    correction_completion: "current_idx"
      #options: current_idx and "as_zero"
        #curr idx: every split gets time indexes fitting to the corresponding part of the trajectory
          #-> all individual basically start at end at the same spots, but vary in form [and previous steps all get cut out]--> parameters mean the same for all
        ############################################################
        # USE current_idx, as_zero CURRENTLY NOT FULLY IMPLEMENTED!#
        ############################################################
        #as_zero: every split will start at time index 0 --> each split starts at the current starting position [all steps after total step size get cut out]
          #-> goal parameter loses direct meaning as goal is not reachable in the used step size but would need last interval start+total_steps

    split_strategy: "fixed_size_rand_start" #n_equal_splits, random_size_range, random_gauss, fixed_sizes, fixed_size_rand_start
    random_permute_splits: False #e.g. ranges = [12,15,36,0] -> permute that list every iteration before assignment, usable for ALL split_strategies

    n_splits: 6        #MOST IMPORTANT: defines how many policy splits will be allowed
                        #also required for split_strategy "n_equal_splits" (if n equal sized are impossible, ITS CURRENTLY NOT SUPPORTED)
    ranges: [50, 35, 10, 5] #required for fixed_sizes --> number of steps assigned for each policy, len(ranged) = n_splits (else error)
    size_range: [15,30]  #required for split_strategy "random_size_range" -> random_size_range takes the splits from the sampler and so from the replay buffer
                          # in the q and v update as splits, so that we dont introduce new gaps
                          # will ensure that there are no splits < size_range[0] (except for first and last if next option==True)
    fixed_size: 20      #size for intermediate splits for fixed_size_rand_start
    size_exception_for_first_and_last_split: True #only for size_range -> default true so that v + q func will be computed at all timesteps
    mean_std: [10,5]    #required for split_strategy "random_gauss"

    q_loss_strategy: "continuing" #"truncated" #q-loss is updated via time-slices --> this determines how the time-sliced get cut for the update
                                 #"truncated" --> one slice only for one parameter segment -> cut off parts that are not created with the same policy parameters
                                 #"start_unchanged" --> each slice has a startpoint from sampling saved in the replay buffer, with this sampling method
                                                      # bigger gaps between two consecutive segments which use different parameters may occur, but the policies start with "known startpoints"
                                 #"continuing" --> take last reached point of previous policy as init point for next segment (parameters are set to fit a different point then)-> may also cause issues
    v_func_estimation: "truncated" #truncated, continuing or "overconfident"
                                  #overconfident: policy which is responsible for init time predicts entire sequence for v-func estimation
  critic:
    type: TopErlCritic
    args:
      bias: true
      n_embd: 128
      block_size: 1024
      dropout: 0.0
      n_layer: 2
      n_head: 8
      update_rate: 0.005
      use_layer_norm: true
      relative_pos: false # false for abs pos encoding, true for relative pos encoding
      single_q: true
      dtype: *dtype
      device: *device

  projection:
    type: KLProjectionLayer  # KL-projection
    args:
      proj_type: kl
      mean_bound: 0.05 #0.005 - 0.01
      cov_bound: 0.0005 #0.00005 - 0.005
      trust_region_coeff: 1.0
      scale_prec: true
      entropy_schedule: linear  # Desired value is linear or exp or None
      target_entropy: 0.0 # target entropy per action dim
      temperature: 0.7
      entropy_eq: false # If the entropy should follow an equality constraint
      entropy_first: false # If the entropy should be the first constraint
      do_regression: false
      dtype: *dtype
      device: *device

  sampler:
    type: TopErlSampler
    args:
      env_id: "fancy_ProDMP_TCE/BoxPushingRandomInitDense-v0"
      dtype: *dtype
      device: *device
      seed: auto
      task_specified_metrics: [ "is_success", "box_goal_pos_dist",
                                "box_goal_rot_dist", "episode_energy" ]
  replay_buffer:
    type: TopErlReplayBuffer
    args:
      buffer_size: 5000
      device: *device
      dtype: *dtype
