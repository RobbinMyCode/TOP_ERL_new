# Global config
exp_name: &exp_name "box_random_seq_entire"
exp_path: &exp_path "../../mprl_exp_result"
sub_exp_name: &sub_exp_name "dense, verify after split the critic and policy updates."
act_func_hidden: &act_func_hidden leaky_relu
act_func_last: &act_func_last null
dtype: &dtype "float32"
device: &device "cuda"
seed: auto

# cw2 config
name: *exp_name
path: *exp_path
verbose_level: 2

# wandb
wandb:
  project: *exp_name
  group: *sub_exp_name
  entity: uiery
  log_interval: &log_interval 100
  log_model: true
  model_name: model

# experiment parameters
params:
  agent:
    type: TopErlAgentMultiProcessing
    # type: TopErlAgent
    args:
      lr_policy: 3e-4
      lr_critic: 5e-5
      wd_policy: 1e-5
      wd_critic: 1e-5
      use_mix_precision: true
      schedule_lr_policy: false
      schedule_lr_critic: false
      entropy_penalty_coef: 0.0
      discount_factor: 1
      epochs_policy: 15
      epochs_critic: 30
      balance_check: *log_interval
      evaluation_interval: *log_interval
      use_old_policy: true  # use a target policy as the old policy for the trust region projection
      old_policy_update_rate: 0.005  # update rate for the old policy from the current policy
      batch_size: 512
      critic_update_from: 1 #300
      policy_update_from: 1 #2000
      dtype: *dtype
      device: *device

  mp: &mp
    type: prodmp
    args:
      num_dof: 7
      tau: 2.0
      alpha_phase: 3
      num_basis: 8
      basis_bandwidth_factor: 3
      num_basis_outside: 0
      alpha: 10
      disable_goal: false
      relative_goal: false
      auto_scale_basis: true
      weights_scale: 0.3
      goal_scale: 0.3
      dt: 0.02
      dtype: *dtype
      device: *device

  policy:
    type: TopErlPolicy
    args:
      mean_net_args:
        avg_neuron: 256
        num_hidden: 2
        shape: 0.0
      variance_net_args:
        std_only: false
        contextual: false
      init_method: orthogonal
      out_layer_gain: 0.01
      min_std: 1e-5
      act_func_hidden: *act_func_hidden
      act_func_last: *act_func_last
      dtype: *dtype
      device: *device
      mp: *mp

  reference_split:
    split_strategy: "random_size_range"
    split_size: 13      #required for split strategy "fixed_max_size" (adds an other smaller split at the end if num_times % split_size != 0
    n_splits: 1         #required for split_strategy "n_equal_splits" (if n equal sized are impossible, adds an other smaller split)
    size_range: [3,20]  #required for split_strategy "random_size_range"
    mean_std: [10,5]    #required for split_strategy "random_gauss"

  critic:
    type: TopErlCritic
    args:
      bias: true
      n_embd: 128
      block_size: 1024
      dropout: 0.0
      n_layer: 2
      n_head: 8
      update_rate: 0.005
      use_layer_norm: true
      relative_pos: false # false for abs pos encoding, true for relative pos encoding
      single_q: true
      dtype: *dtype
      device: *device

  projection:
    type: KLProjectionLayer  # KL-projection
    args:
      proj_type: kl
      mean_bound: 0.05
      cov_bound: 0.0005
      trust_region_coeff: 1.0
      scale_prec: true
      entropy_schedule: linear  # Desired value is linear or exp or None
      target_entropy: 0.0 # target entropy per action dim
      temperature: 0.7
      entropy_eq: false # If the entropy should follow an equality constraint
      entropy_first: false # If the entropy should be the first constraint
      do_regression: false
      dtype: *dtype
      device: *device

  sampler:
    type: TopErlSampler
    args:
      env_id: "fancy_ProDMP_TCE/BoxPushingRandomInitDense-v0"
      dtype: *dtype
      device: *device
      seed: auto
      task_specified_metrics: [ "is_success", "box_goal_pos_dist",
                                "box_goal_rot_dist", "episode_energy" ]
  replay_buffer:
    type: TopErlReplayBuffer
    args:
      buffer_size: 5000
      device: *device
      dtype: *dtype
