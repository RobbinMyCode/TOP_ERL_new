# Global config
exp_name: &exp_name "box_random_seq_entire"
exp_path: &exp_path "../../mprl_exp_result"
sub_exp_name: &sub_exp_name "fix_n=2_no_initcond_adapt_lr_tune"
act_func_hidden: &act_func_hidden leaky_relu
act_func_last: &act_func_last null
dtype: &dtype "float32"
device: &device "cuda"
seed: auto

# cw2 config
name: *exp_name
path: *exp_path
verbose_level: 2

# wandb
wandb:
  project: *exp_name
  group: *sub_exp_name
  entity: uiery
  log_interval: &log_interval 100
  log_model: true
  model_name: model

# experiment parameters
params:
  agent:
    type: TopErlAgentMultiProcessing
    # type: TopErlAgent
    args:
      lr_policy: 3e-4   #grid with lr critic
      lr_critic: 5e-5 #grid with lr policy
      wd_policy: 1e-5
      wd_critic: 1e-5
      use_mix_precision: true
      schedule_lr_policy: false
      schedule_lr_critic: false
      entropy_penalty_coef: 0.0
      discount_factor: 1
      epochs_policy: 30 #10-50 --> increase
      epochs_critic: 30 #10-50 --> fine like this
      balance_check: *log_interval
      evaluation_interval: *log_interval
      use_old_policy: true  #can also be tried as false (but late) use a target policy as the old policy for the trust region projection
      old_policy_update_rate: 0.005  # update rate for the old policy from the current policy
      batch_size: 512
      critic_update_from: 300 #1 # 300
      policy_update_from: 2000 #150 # 2000
      dtype: *dtype
      device: *device

  mp: &mp
    type: prodmp #prodmp , "prodmp_reuse_sample" if for each trajectory the same position in distrs should be sampled
    args:
      num_dof: 7
      tau: 2.0
      alpha_phase: 3
      num_basis: 8
      basis_bandwidth_factor: 3
      num_basis_outside: 0
      alpha: 10
      disable_goal: false
      relative_goal: false
      auto_scale_basis: true
      weights_scale: 0.3
      goal_scale: 0.3
      dt: 0.02
      dtype: *dtype
      device: *device

  policy:
    type: TopErlPolicy
    include_pos_in_forcing_terms: false
    args:
      mean_net_args:
        avg_neuron: 256
        num_hidden: 2
        shape: 0.0
      variance_net_args:
        std_only: false
        contextual: false
      init_method: orthogonal
      out_layer_gain: 0.01
      min_std: 1e-5
      act_func_hidden: *act_func_hidden
      act_func_last: *act_func_last
      dtype: *dtype
      device: *device
      mp: *mp

  reference_split:
    correction_completion: "current_idx"
      #options: current_idx and "as_zero"
        #curr idx: every split gets time indexes fitting to the corresponding part of the trajectory
          #-> all individual basically start at end at the same spots, but vary in form [and previous steps all get cut out]--> parameters mean the same for all
        ############################################################
        # USE current_idx, as_zero CURRENTLY NOT FULLY IMPLEMENTED!#
        ############################################################
        #as_zero: every split will start at time index 0 --> each split starts at the current starting position [all steps after total step size get cut out]
          #-> goal parameter loses direct meaning as goal is not reachable in the used step size but would need last interval start+total_steps

    split_strategy: "n_equal_splits" #n_equal_splits, random_size_range, random_gauss, fixed_sizes, fixed_size_rand_start, intra_episode_fixed_inter_rand_size  _rand_semi_fixed_size_focus_on_region
    random_permute_splits: False #e.g. ranges = [12,15,36,0] -> permute that list every iteration before assignment, usable for ALL split_strategies

    n_splits: 2              #also required for split_strategy "n_equal_splits"
    ranges: [50, 35, 10, 5] #required for fixed_sizes --> number of steps assigned for each policy, len(ranged) = n_splits (else error)
    size_range: [15,30]  #required for split_strategy "random_size_range" -> random_size_range takes the splits from the sampler and so from the replay buffer
                          # in the q and v update as splits, so that we dont introduce new gaps
                          # will ensure that there are no splits < size_range[0] (except for first and last if next option==True)
    fixed_size: 99 #tune from [5,10,20,30,35] 5-35      #size for intermediate splits for fixed_size_rand_start

    use_top_erl_splits_policy: True #if true updates policy for random splits also with random_segments instead of from each predefined segment
    use_top_erl_splits_critic: True #analog to policy but for critic update
    policy_use_all_action_indexes: False #ONLY FOR TOP_ERL_UPDATES, if True -> add a smaller segment at the end of the random segments if it does not include action with last index

    ignore_top_erl_updates_after_index: 9999

    inter_fixed_size_range: [20, 35] #required for intra_episode_fixed_inter_rand_size
                            #--> works like fixed_size_rand_start but for each trajectory fixed_size in sampled in between the two values
    focus_regions: [30, 80] #required for rand_semi_fixed_size_focus_on_region:  each value gives a separate region
                            #[focus_region[i]-focus_regions_size_1way, focus_region[i]+focus_regions_size_1way],
                            #around where the sampling size is decreased
                            #-- uniformly sampled in [min_decreased_size, fixed_size] (include_last=True)
    focus_regions_size_1way: 15 #required for rand_semi_fixed_size_focus_on_region: (half-)size of focus region
    min_decreased_size: 10  #required for: rand_semi_fixed_size_focus_on_region:    min sampling distance in focus region
    size_exception_for_first_and_last_split: True #only for size_range -> default true so that v + q func will be computed at all timesteps
    mean_std: [10,5]    #required for split_strategy "random_gauss"

    q_loss_strategy: "enforce_no_overlap_overconf_include_0" #"truncated" #q-loss is updated via time-slices --> this determines how the time-sliced get cut for the update
                                 #"truncated" --> one slice only for one parameter segment -> cut off parts that are not created with the same policy parameters
                                 #"start_unchanged" --> each slice has a startpoint from sampling saved in the replay buffer, with this sampling method
                                                      # bigger gaps between two consecutive segments which use different parameters may occur, but the policies start with "known startpoints"
                                 #"continuing" --> take last reached point of previous policy as init point for next segment (parameters are set to fit a different point then)-> may also cause issues
    v_func_estimation: "overconfident" #truncated, continuing or "overconfident"
                                  #overconfident: policy which is responsible for init time predicts entire sequence for v-func estimation


    re_use_rand_coord_from_sampler_for_updates: False #only for prodmp_reuse_sample
  critic:
    type: TopErlCritic
    args:
      bias: true
      n_embd: 128
      block_size: 1024
      dropout: 0.0
      n_layer: 2
      n_head: 8
      update_rate: 0.005
      use_layer_norm: true
      relative_pos: false # false for abs pos encoding, true for relative pos encoding
      single_q: true
      dtype: *dtype
      device: *device

  projection:
    type: KLProjectionLayer  # KL-projection
    args:
      proj_type: kl
      mean_bound: 0.05    #increase in case *=2-5
      cov_bound: 0.0005   #increase in case *=2-5
      trust_region_coeff: 1.0
      scale_prec: true
      entropy_schedule: linear  # Desired value is linear or exp or None
      target_entropy: 0.0 # target entropy per action dim
      temperature: 0.7
      entropy_eq: false # If the entropy should follow an equality constraint
      entropy_first: false # If the entropy should be the first constraint
      do_regression: false
      dtype: *dtype
      device: *device

  sampler:
    type: TopErlSampler
    args:
      env_id: "fancy_ProDMP_TCE/BoxPushingRandomInitDense-v0"
      dtype: *dtype
      device: *device
      seed: auto
      task_specified_metrics: [ "is_success", "box_goal_pos_dist",
                                "box_goal_rot_dist", "episode_energy" ]
  replay_buffer:
    type: TopErlReplayBuffer
    args:
      buffer_size: 5000 #2000-7000 later tries
      device: *device
      dtype: *dtype
